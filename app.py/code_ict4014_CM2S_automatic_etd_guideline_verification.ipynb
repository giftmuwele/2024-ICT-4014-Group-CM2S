{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "  <font size=6><b>Electronic Thesis and Dissertations Automatic Guidelines Verification\n",
        "</b></font>\n",
        "\n",
        "   Mubanga Chibesa 2020031345@student.unza.zm<sup>1</sup> Albertina Mooka 2020019299@student.unza.zm<sup>2</sup> Gift Muwele 2020014203@student.unza.zm<sup>2</sup> and Lwiime Shansonga 2020002353@student.unza.zm <sup>4</sup>\n",
        "\n",
        "  <sup>1</sup> Department of Library and Information Science, University of Zambia, P.O Box 32379, Lusaka, Zambia</br>\n",
        "\n",
        "  Supervisor: Dr.Lighton Phiri lighton.phiri@unza.zm\n",
        "  <br/>\n",
        "\n",
        "  August 2024\n",
        "</center>\n",
        "\n",
        "<justify>\n",
        "Higher Education Institutions worldwide enforce guidelines and academic approaches to ensure scholarly integrity and adherence to academic standards.The University of Zambia, is not an exception. like most HEIs it offers  training to postgraduate students and one of the key aspects of postgraduate training is the production of an Electronic Thesis and Dissertation manuscript. The Directorate of Research and Graduate Studies (DRGS) of the University of Zambia provides guidelines which stipulate how ETD’s should be formatted. However, the process of  checking for conformance is a manual and tedious process, resulting in submission of inconsistently formatted manuscripts in the Institutional Repository (UNZA IR).\n",
        "To address this challenge, we embarked on a  project to implement a tool that will automate the process of checking ETD’s compliance against established postgraduate guidelines, leveraging data mining techniques to perform these tasks. More specifically Document Layout Analysis (DLA) as the core approach used in  the implementation. The tool flags off portions of ETD manuscripts that do not  conform to stipulated guidelines.\n",
        "Using a mixed methods approach, document analysis was employed to understand the postgraduate guidelines stipulated in the “Regulations and Guidelines for Postgraduate Studies” guidelines document; content analysis was  used to randomly sampled ETDs in order to experimentally determine the extent of the problem and, finally interviews and questionairs were used to find out the challenges that postgraduates, Alumni and Lecturers encounter during the process.\n",
        "Upon successful completion of this project, we anticipate a reduced workload associated with the process of manually checking if manuscripts conform to postgraduate guidelines, freeing up time and resources to handle other academic tasks like attending more to the content of the manuscripts. Not only that, academic integrity is consequently going to be improved through the standardisation of ETD formats and this will add to the university’s reputation and credibility.\n",
        "</justify>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qHDnitGxzrcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proposed Solution**\n",
        "\n",
        " To implement a tool that will automate the process of checking the compliance of ETDs to postgraduate guidelines and consequently enhance the compliance levels of ETDs published on the University of Zambia's institutional repository thereafter. The tool will have a focus on improving the efficiency of the document  review process by employing data mining techniques, specifically Document Layout Analysis (DLA) Strategies in the implementation of this pipeline. The key feature of this tool is evaluating the layout of ETDs against institutional guidelines and flagging off portions of these manuscripts that do not conform to the guidelines\n",
        "\n",
        " This Notebook outlines how data mining techniques were utilised to flag fo portions of manuscripts that do not conform to the postgraduate guidelines."
      ],
      "metadata": {
        "id": "P3hiRe4UOYqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Methodology**"
      ],
      "metadata": {
        "id": "CzWP-7LXQBKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Packages and Libraries Imports**"
      ],
      "metadata": {
        "id": "KoAig-tFKfmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Libraries and Packages Imports\n",
        "!apt-get install poppler-utils\n",
        "!pip install --upgrade deepdoctection\n",
        "!pip install deepdoctection\n",
        "!pip install Tensorflow\n",
        "!pip install pytorch\n",
        "!apt-get install tesseract-ocr\n",
        "!pip install pdfplumber\n",
        "!pip install pdf2image\n",
        "\n",
        "import deepdoctection as dd\n",
        "from pdf2image import convert_from_path\n",
        "from pathlib import Path\n",
        "from matplotlib import pyplot as plt\n",
        "from pathlib import Path\n",
        "from deepdoctection import get_dd_analyzer\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "import json\n",
        "analyzer = dd.get_dd_analyzer(config_overwrite=[\"LANGUAGE='eng'\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FM2zEsE7i0G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An API that will be linking the frontend and backend using flask."
      ],
      "metadata": {
        "id": "qtz3u92emjph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask"
      ],
      "metadata": {
        "id": "LycV2QgKqEwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "@app.after_request\n",
        "def after_request(response):\n",
        "    response.headers.add('Access-Control-Allow-Origin', '*')\n",
        "    response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')\n",
        "    response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE')\n",
        "    return response\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Example GET endpoint\n",
        "@app.route('/api/upload', methods=['GET'])\n",
        "def handle-upload():\n",
        "    return jsonify(message='Hello from the backend!')\n",
        "\n",
        "# Example POST endpoint\n",
        "@app.route('/api/data', methods=['POST'])\n",
        "def receive_data():\n",
        "    data = request.json\n",
        "    name = data.get('name', 'Unknown')\n",
        "    return jsonify(message=f'Hello, {name}!')\n",
        "\n",
        "# New endpoint to handle file uploads\n",
        "@app.route('/api/upload', methods=['POST'])\n",
        "def upload_file():\n",
        "    # Check if a file is part of the request\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify(message='No file part in the request'), 400\n",
        "\n",
        "    file = request.files['file']\n",
        "\n",
        "    # Check if a file was submitted\n",
        "    if file.filename == '':\n",
        "        return jsonify(message='No file selected'), 400\n",
        "\n",
        "    # Save file (optional)\n",
        "    # file.save(f'/path/to/save/{file.filename}')\n",
        "\n",
        "    # Perform any processing needed with the file\n",
        "    file_content = file.read()  # Example: read file content\n",
        "\n",
        "    return jsonify(message=f'File {file.filename} uploaded successfully!', content=str(file_content))\n",
        "\n",
        "# Run the Flask app\n",
        "if __name__ == '__main__':\n",
        "    app.run(port=3000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grIprv2lqkDo",
        "outputId": "ce90953e-fd4c-48c0-f6e5-2095eebfbafd"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:3000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Document Format Compliance Verification**"
      ],
      "metadata": {
        "id": "vE73WhonQ8Eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_file = next(iter(uploaded)) # Access uploaded pdf file\n",
        "\n",
        "# Set PDF file path\n",
        "pdf_path = Path(r\"ETD-ED1.pdf\")\n",
        "\n",
        "# Convert PDF to a list of images (one per page)\n",
        "pages = convert_from_path(pdf_path, dpi=300)\n",
        "\n",
        "# Check if the document pages are less than 150\n",
        "if len(pages) >= 150:\n",
        "    print(\"The document has 150 or more pages.\")\n",
        "else:\n",
        "    print(\"The document has fewer than 150 pages.\")\n",
        "\n",
        "# Display the first page or any other page by indexing into `pages`)\n",
        "plt.figure(figsize=(25, 17))\n",
        "plt.axis('off')\n",
        "plt.imshow(pages[0])  # pages[0] corresponds to the first page of the PDF\n",
        "plt.show()\n",
        "\n",
        "# Process each page image using the analyzer\n",
        "results = []\n",
        "for i, page_image in enumerate(pages):\n",
        "    # Convert the PIL image to a format compatible with deepdoctection\n",
        "    page_image_path = Path(f\"page_{i}.png\")\n",
        "    page_image.save(page_image_path)  # file save to unique path for each page\n",
        "\n",
        "    # Analyze the image\n",
        "    analysis_result = analyzer.analyze(path=pdf_path, page_number=1+1)\n",
        "    results.append(analysis_result)\n",
        "    print(f\"Analysis result for page {i}:\", analysis_result)\n",
        "\n",
        "    # Check font size for each page (assumed field in analysis_result)\n",
        "    if 'font_size' in analysis_result:\n",
        "        font_size = analysis_result['font_size']\n",
        "        print(f\"Page {i} font size:\", font_size)\n",
        "\n",
        "        if font_size != 12:\n",
        "            print(f\"Warning: Page {i} does not have the correct font size. Expected 12pt.\")\n",
        "    else:\n",
        "        print(f\"Font size information not available for page {i}.\")\n",
        "\n",
        "#Check for abstract content\n",
        "    if 'abstract' in analysis_result:\n",
        "        abstract_content = analysis_result['abstract']\n",
        "        abstract_length_chars = len(abstract_content)  # Count the number of characters\n",
        "\n",
        "        # If 'abstract' is structured as a list (one entry per page), count its length\n",
        "        if isinstance(abstract_content, list):\n",
        "            abstract_length_pages = len(abstract_content)\n",
        "            abstract_text_combined = \" \".join(abstract_content)  # Combine all parts for character count\n",
        "            abstract_length_chars = len(abstract_text_combined)\n",
        "        else:\n",
        "            abstract_length_pages = 1  # Assume single page if not a list\n",
        "\n",
        "        print(f\"Abstract length: {abstract_length_pages} page(s), {abstract_length_chars} characters\")\n",
        "\n",
        "        # Check if the abstract is exactly one page or 300 characters long\n",
        "        if abstract_length_pages == 1 or abstract_length_chars == 300:\n",
        "            print(\"The abstract meets the requirement of being one page long or exactly 300 characters.\")\n",
        "        else:\n",
        "            print(\"Warning: The abstract does not meet the requirement of being one page long or exactly 300 characters.\")\n",
        "    else:\n",
        "        print(f\"Abstract content not found in analysis result for page {i}.\")\n",
        "\n",
        "\n",
        "# Save results as a JSON file\n",
        "with open(\"analysis_results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=4)\n"
      ],
      "metadata": {
        "id": "O_hsmItV1CQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3kvDstNmMmFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IRblqUh-xapf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3fcXFfi_Mjm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DbIFTlY0zoqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7NyyWrwYwrD4"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}